{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# RNN を使ったテキスト分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "# !pip install -q tf-nightly\n",
    "# import tensorflow_datasets as tfds\n",
    "# !pip3 list | grep tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "# !pip3 install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbj_names = [\n",
    "    # ['Eigo']\n",
    "    ['Suugaku']\n",
    "    # ['Eigo', 'Suugaku']\n",
    "][0]\n",
    "elmnt_name = 'question'\n",
    "attr_name =  'answer_type'   # 'knowledge_type'  #\n",
    "is_remove_xml = False\n",
    "\n",
    "df = None\n",
    "for sbj in sbj_names:\n",
    "    print(sbj)\n",
    "    attr_csv_path = f'../{sbj}_{attr_name}_ds.tsv'\n",
    "    df_tmp = pd.read_csv(attr_csv_path, delimiter='\\t')\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()  # nan を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XMLタグ除去\n",
    "if is_remove_xml:\n",
    "    import re\n",
    "    def remove_xml(xml_child_str):\n",
    "        xml_child_str = re.sub('<.*?>', '', xml_child_str)\n",
    "        return re.sub('</.*?>', '', xml_child_str)\n",
    "\n",
    "    # test_idx = 200\n",
    "    # print(df['contents'][test_idx])\n",
    "    # print(remove_xml(df['contents'][test_idx]))\n",
    "    df['contents'] = df['contents'].map(remove_xml)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### Tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dir = '_logs/SentencePiece'\n",
    "os.makedirs(m_dir, exist_ok=True)\n",
    "df.to_csv(f'{m_dir}/tmp.txt', sep='\\t')\n",
    "\n",
    "# arg_str = '--input={m_dir}/tmp.txt --model_prefix={m_dir}/m_user ' + '--user_defined_symbols=<sep>,<cls>' + ',<ansColumn/>,<label>' + ' --vocab_size=2000'\n",
    "# spm.SentencePieceTrainer.train(arg_str)\n",
    "\n",
    "spm.SentencePieceTrainer.train(f'--input={m_dir}/tmp.txt --model_prefix={m_dir}/m  --user_defined_symbols=<sep>,<cls>,<pad>   --vocab_size=2000')\n",
    "sp = spm.SentencePieceProcessor()  # model_file='SentencePiece/test_model.model'\n",
    "\n",
    "sp.load(f'{m_dir}/m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode: text => id\n",
    "# tokenized_tokens =  sp.encode_as_pieces('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "# print(tokenized_tokens)\n",
    "\n",
    "# tokenized_ids = sp.encode_as_ids('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "# print(tokenized_ids)\n",
    "\n",
    "# decoded_text = sp.decode(tokenized_ids)\n",
    "# print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_content = df_tmp['contents'][20]\n",
    "# print(example_content, sp.encode_as_pieces(example_content))\n",
    "\n",
    "# for index in encoded_string:\n",
    "#   print('{} ----> {}'.format(index, encoder.decode([index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlYWqhTVlUyQ"
   },
   "source": [
    "## Train 用データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VznrltNOnUc5"
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "# 系列を揃えるためのパディング文字列<pad>を追加\n",
    "# パディング文字列のIDは0とする\n",
    "word2index.update({\"<pad>\":0})\n",
    "\n",
    "for inst, cont in zip(df['<instruction/>'], df['contents']):\n",
    "#     try:\n",
    "    tokens = sp.encode_as_pieces(inst + cont)\n",
    "    for word in tokens:\n",
    "            if word in word2index: continue\n",
    "            word2index[word] = len(word2index)\n",
    "#     except TypeError:\n",
    "#         print(f'[Error] <instruction/> が nan です。')\n",
    "#         print(f'    inst : {inst}')\n",
    "#         print(f'    cont : {cont}')\n",
    "\n",
    "print(\"vocab size : \", len(word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set_dict から自動抽出する！\n",
    "categories = set()\n",
    "for sbj in sbj_names:\n",
    "    with open(f'../class_set/{sbj}-{elmnt_name}-{attr_name}.json') as f:\n",
    "        categories |= set(json.load(f))   # sbj_names = ['Eigo', ]\n",
    "\n",
    "categories = list(categories)\n",
    "categories.sort()    # 入れないと、クラス番号が変わってしまい、再現実験ができないので注意？\n",
    "print(categories)\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lgLRE0z4Opm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 系列の長さを揃えてバッチでまとめる\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "cat2index = {}\n",
    "for cat in categories:\n",
    "    if cat in cat2index: continue\n",
    "    cat2index[cat] = len(cat2index)\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    # print(tokens)\n",
    "    return [word2index[w] for w in tokens]\n",
    "\n",
    "def category2index(cat):\n",
    "    return cat2index[cat]\n",
    "\n",
    "index_datasets_c_xml_tmp = []\n",
    "index_datasets_category = []\n",
    "\n",
    "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
    "max_len = 0\n",
    "for inst, cont, category in tqdm(zip(df['<instruction/>'], df['contents'], df[attr_name])):\n",
    "    index_c_xml = sentence2index(inst + cont)\n",
    "    index_category = category2index(category)\n",
    "    index_datasets_c_xml_tmp.append(index_c_xml)\n",
    "    index_datasets_category.append(index_category)\n",
    "    if max_len < len(index_c_xml):\n",
    "        max_len = len(index_c_xml)\n",
    "        # if max_len > 10000:\n",
    "        #     print(inst, cont)\n",
    "\n",
    "# 系列の長さを揃えるために短い系列にパディングを追加\n",
    "index_datasets_c_xml = []\n",
    "for c_xml in tqdm(index_datasets_c_xml_tmp):\n",
    "    # パディング作成\n",
    "    padd = [0] * (max_len - len(c_xml))\n",
    "    # 後ろパディングだと正しく学習できなかったので、前パディング\n",
    "    c_xml = padd + c_xml # 前パディング\n",
    "    # c_xml = c_xml + padd # 後ろパディング\n",
    "#     print(len(c_xml))\n",
    "    index_datasets_c_xml.append(c_xml)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(index_datasets_c_xml, index_datasets_category, train_size=0.7)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "# print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# 特徴量の標準化\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_valid = scaler.transform(x_valid)\n",
    "\n",
    "# Tensor型に変換\n",
    "# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_valid = torch.from_numpy(x_valid)\n",
    "y_valid = torch.from_numpy(y_valid)\n",
    "\n",
    "print('x_train : ', x_train.shape)\n",
    "print('y_train : ', y_train.shape)\n",
    "print('x_valid : ', x_valid.shape)\n",
    "print('y_valid : ', y_valid.shape)\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Dataset  ###\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "# 動作確認\n",
    "# indexを指定すればデータを取り出すことができます。\n",
    "index = 0\n",
    "print(train_dataset.__getitem__(index)[0].size())\n",
    "print(train_dataset.__getitem__(index)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "from model_abc.LSTM_text_classify_model import (\n",
    "    LSTM_TextClassifier_ptModel\n",
    ")\n",
    "%autoreload\n",
    "\n",
    "# GPUを使うために必要\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "# モデルのハイパーパラメータ\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_SIZE = len(categories)\n",
    "\n",
    "## モデルの保存場所を準備する。\n",
    "import datetime\n",
    "dt_now = datetime.datetime.now()\n",
    "save_m_dir = os.path.join('_logs', dt_now.strftime('%m%d_%Hh%Mm%Ss'))\n",
    "\n",
    "model = LSTM_TextClassifier_ptModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE,\n",
    "                                        save_m_dir, save_m_file='LSTM_classifier_.pth').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwH3nto596k"
   },
   "source": [
    "## Experiment Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from base_ExpTrain import Batch_ExpTrain\n",
    "\n",
    "# 実験設定\n",
    "lr=0.001\n",
    "epochs=1500\n",
    "batch_size=50\n",
    "early_stopping=800\n",
    "\n",
    "# exec\n",
    "exp_batch_train = Batch_ExpTrain(train_dataset, valid_dataset, device)\n",
    "criterion = nn.NLLLoss()  # ignore_index=PAD_token\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "max_valid_acc = exp_batch_train.exec(\n",
    "                            model, criterion, optimizer,\n",
    "                            epochs=epochs, batch_size=batch_size, early_stopping=early_stopping )\n",
    "                            # teacher_forcing=0.5, early_stopping=5)\n",
    "print(max_valid_acc)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 実験パラメータのメモを保存\n",
    "import json\n",
    "\n",
    "with open(os.path.join(save_m_dir, 'model_params.json'), 'w') as param_f:\n",
    "    json.dump({\n",
    "        'Model' : {\n",
    "            'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "            'HIDDEN_DIM' : HIDDEN_DIM,\n",
    "            'VOCAB_SIZE' : VOCAB_SIZE,\n",
    "            'TAG_SIZE':TAG_SIZE,\n",
    "        },\n",
    "        'Data' : {\n",
    "            'クラス数' : len(categories),\n",
    "            \"vocab size\" : len(word2index),\n",
    "            'max_len' : max_len,\n",
    "            'train' : {\n",
    "                'データ数' : len(train_dataset),\n",
    "            },\n",
    "            'valid' : {\n",
    "                'データ数' : len(valid_dataset),\n",
    "            }\n",
    "        },\n",
    "        'Experiment' : {\n",
    "            'XML条件' : {\n",
    "                'sbj_names': sbj_names,\n",
    "                'elmnt_name': elmnt_name,\n",
    "                'attr_name': attr_name,\n",
    "                'is_remove_xml' : is_remove_xml\n",
    "            },\n",
    "           '実験設定' : {\n",
    "                'lr':lr,\n",
    "                'epochs':epochs,\n",
    "                'batch_size':batch_size,\n",
    "                'early_stopping':early_stopping\n",
    "            },\n",
    "            '実験結果' : {\n",
    "                'max_valid_acc': max_valid_acc,\n",
    "                'valid負例x100' : []\n",
    "            }\n",
    "        },\n",
    "    }, param_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Acc 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaNbXi43YgUT"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "valid_acc = 0\n",
    "total_count = 0\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (X_batch, Y_batch) in enumerate(valid_dataloader):\n",
    "        valid_loss = 0\n",
    "        valid_loss, pred_batch_arr = model.predict(X_batch, Y_batch, criterion, device)\n",
    "        # acc を計算する。\n",
    "        _, pred_batch = torch.max(pred_batch_arr, 1)\n",
    "        # acc を計算する。\n",
    "        for j, ans in enumerate(Y_batch):\n",
    "            # print(pred_batch[j].item(), ans.item())\n",
    "            if pred_batch[j].item() == ans.item():\n",
    "                valid_acc += 1\n",
    "#             else:\n",
    "#                 print(predicts[j].item(), ans.item())\n",
    "        total_count += Y_batch.size(0)\n",
    "    valid_acc /= total_count\n",
    "\n",
    "print(\"[Info] acc : \", valid_acc, \"loss : \", valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('tensorflow2.3_py3.8': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}