{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# RNN を使ったテキスト分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "# !pip install -q tf-nightly\n",
    "# import tensorflow_datasets as tfds\n",
    "# !pip3 list | grep tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "# !pip3 install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigo\n",
      "Suugaku\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>knowledge_type</th>\n",
       "      <th>&lt;instruction/&gt;</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問１&lt;/label&gt; &lt;ansColumn id=\"A1\"&gt;1&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問２&lt;/label&gt; &lt;ansColumn id=\"A2\"&gt;2&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問３&lt;/label&gt; &lt;ansColumn id=\"A3\"&gt;3&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問４&lt;/label&gt; &lt;ansColumn id=\"A4\"&gt;4&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問５&lt;/label&gt; &lt;ansColumn id=\"A5\"&gt;5&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>393</td>\n",
       "      <td>MATH_IIB_VECTOR</td>\n",
       "      <td>三つのベクトル，，について …………………………① …………………………②ア，イに当てはまる...</td>\n",
       "      <td>&lt;label&gt;(2)&lt;/label&gt;  &lt;instruction&gt;&lt;formula /&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>394</td>\n",
       "      <td>MATH_IIB_VECTOR</td>\n",
       "      <td>により，三角形ABCは正三角形である。以下，4点A，B，C，Dが，正四面体の四つの頂点になる...</td>\n",
       "      <td>&lt;label&gt;(3)&lt;/label&gt;  &lt;instruction&gt;&lt;formula&gt;(x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>395</td>\n",
       "      <td>IC_O,IC_T,MATH_IIB_STATISTICS</td>\n",
       "      <td>0＜p＜1とする。袋の中に白球がp，赤球が1-pの割合で，全部でm個入っているものとする...</td>\n",
       "      <td>&lt;label&gt;(1)&lt;/label&gt;  &lt;instruction&gt;&lt;formula /&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>396</td>\n",
       "      <td>IC_O,IC_T,MATH_IIB_STATISTICS</td>\n",
       "      <td>とする。この袋の中から1個の球を取り出し袋の中へ戻すという試行を4回繰り返すとき，白球の出る...</td>\n",
       "      <td>&lt;label&gt;(2)&lt;/label&gt;  &lt;instruction&gt;&lt;formula&gt;m=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>397</td>\n",
       "      <td>IC_O,IC_T,MATH_IIB_STATISTICS</td>\n",
       "      <td>m=10，とする。この袋の中から同時に4個の球を取り出すとき，白球の個数を表す確率変数をYと...</td>\n",
       "      <td>&lt;label&gt;(3)&lt;/label&gt;  &lt;instruction&gt;以下では，&lt;formu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3016 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                 knowledge_type  \\\n",
       "0              0                            PRN   \n",
       "1              1                            PRN   \n",
       "2              2                            PRN   \n",
       "3              3                            PRN   \n",
       "4              4                            PRN   \n",
       "...          ...                            ...   \n",
       "3028         393                MATH_IIB_VECTOR   \n",
       "3029         394                MATH_IIB_VECTOR   \n",
       "3030         395  IC_O,IC_T,MATH_IIB_STATISTICS   \n",
       "3031         396  IC_O,IC_T,MATH_IIB_STATISTICS   \n",
       "3032         397  IC_O,IC_T,MATH_IIB_STATISTICS   \n",
       "\n",
       "                                         <instruction/>  \\\n",
       "0     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "1     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "2     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "3     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "4     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "...                                                 ...   \n",
       "3028  三つのベクトル，，について …………………………① …………………………②ア，イに当てはまる...   \n",
       "3029  により，三角形ABCは正三角形である。以下，4点A，B，C，Dが，正四面体の四つの頂点になる...   \n",
       "3030    0＜p＜1とする。袋の中に白球がp，赤球が1-pの割合で，全部でm個入っているものとする...   \n",
       "3031  とする。この袋の中から1個の球を取り出し袋の中へ戻すという試行を4回繰り返すとき，白球の出る...   \n",
       "3032  m=10，とする。この袋の中から同時に4個の球を取り出すとき，白球の個数を表す確率変数をYと...   \n",
       "\n",
       "                                               contents  \n",
       "0       <label>問１</label> <ansColumn id=\"A1\">1</ansC...  \n",
       "1       <label>問２</label> <ansColumn id=\"A2\">2</ansC...  \n",
       "2       <label>問３</label> <ansColumn id=\"A3\">3</ansC...  \n",
       "3       <label>問４</label> <ansColumn id=\"A4\">4</ansC...  \n",
       "4       <label>問５</label> <ansColumn id=\"A5\">5</ansC...  \n",
       "...                                                 ...  \n",
       "3028    <label>(2)</label>  <instruction><formula />...  \n",
       "3029    <label>(3)</label>  <instruction><formula>(x...  \n",
       "3030    <label>(1)</label>  <instruction><formula />...  \n",
       "3031    <label>(2)</label>  <instruction><formula>m=...  \n",
       "3032    <label>(3)</label>  <instruction>以下では，<formu...  \n",
       "\n",
       "[3016 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbj_names = [\n",
    "    # ['Eigo']\n",
    "    ['Suugaku']\n",
    "    # ['Eigo', 'Suugaku']\n",
    "][0]\n",
    "elmnt_name = 'question'\n",
    "attr_name = 'knowledge_type'  # 'answer_type'   #\n",
    "\n",
    "df = None\n",
    "for sbj in sbj_names:\n",
    "    print(sbj)\n",
    "    attr_csv_path = f'../{sbj}_{attr_name}_ds.tsv'\n",
    "    df_tmp = pd.read_csv(attr_csv_path, delimiter='\\t')\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()  # nan を削除\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### Tokenize\n",
    "SentencePiece を使用。\n",
    "- タグ あり／なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SHRwRoP2nVHX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>knowledge_type</th>\n",
       "      <th>&lt;instruction/&gt;</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>１～４)の空欄( 42 ～ 45 )に入れるのに最も適当なものを，それぞれ以下の①～④のうち...</td>\n",
       "      <td>&lt;label&gt;問１&lt;/label&gt;&lt;data id=\"D29\" type=\"text\"&gt;Wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2351</th>\n",
       "      <td>2351</td>\n",
       "      <td>DIS_W,R_ENT</td>\n",
       "      <td>次の文章を読み，下の問い（Ａ・Ｂ）に答えよ。なお，文章の左にある(1)～(6)は段落の番...</td>\n",
       "      <td>&lt;label&gt;問５&lt;/label&gt;&lt;data id=\"D48\" type=\"text\"&gt; T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>1431</td>\n",
       "      <td>DIS_S</td>\n",
       "      <td>次の問い(問１・問２)において，文章の 28 ・ 29 に入れる三つの文が，順不同で以下のA...</td>\n",
       "      <td>&lt;label&gt;問１&lt;/label&gt;  &lt;data id=\"D22\" type=\"text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>475</td>\n",
       "      <td>DIS_S, IC_O</td>\n",
       "      <td>(配点 16)</td>\n",
       "      <td>&lt;label&gt;問３&lt;/label&gt;&lt;ansColumn id=\"A44\"&gt;44&lt;/ansCo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>532</td>\n",
       "      <td>DIS_S, R_ENT</td>\n",
       "      <td>(配点 24)</td>\n",
       "      <td>&lt;label&gt;問３&lt;/label&gt;&lt;data id=\"D43\" type=\"text\"&gt;Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 knowledge_type  \\\n",
       "270          270           R_QA   \n",
       "2351        2351    DIS_W,R_ENT   \n",
       "1431        1431          DIS_S   \n",
       "475          475    DIS_S, IC_O   \n",
       "532          532   DIS_S, R_ENT   \n",
       "\n",
       "                                         <instruction/>  \\\n",
       "270   １～４)の空欄( 42 ～ 45 )に入れるのに最も適当なものを，それぞれ以下の①～④のうち...   \n",
       "2351    次の文章を読み，下の問い（Ａ・Ｂ）に答えよ。なお，文章の左にある(1)～(6)は段落の番...   \n",
       "1431  次の問い(問１・問２)において，文章の 28 ・ 29 に入れる三つの文が，順不同で以下のA...   \n",
       "475                                            (配点 16)    \n",
       "532                                            (配点 24)    \n",
       "\n",
       "                                               contents  \n",
       "270   <label>問１</label><data id=\"D29\" type=\"text\">Wh...  \n",
       "2351  <label>問５</label><data id=\"D48\" type=\"text\"> T...  \n",
       "1431    <label>問１</label>  <data id=\"D22\" type=\"text...  \n",
       "475   <label>問３</label><ansColumn id=\"A44\">44</ansCo...  \n",
       "532   <label>問３</label><data id=\"D43\" type=\"text\">Th...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_examples, test_examples = train_test_split(\n",
    "                                    df, test_size=0.4, random_state=0)\n",
    "train_examples.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'次の一連の文章(問１・２)の中の①～③および④～⑥には，それぞれ強く発音されるべき語が一つずつある。その語を選べ。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp = df[['<instruction/>', 'contents']]\n",
    "df_tmp['<instruction/>'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_dir = '_logs/SentencePiece'\n",
    "os.makedirs(m_dir, exist_ok=True)\n",
    "df.to_csv(f'{m_dir}/tmp.txt', sep='\\t')\n",
    "\n",
    "# arg_str = '--input={m_dir}/tmp.txt --model_prefix={m_dir}/m_user ' + '--user_defined_symbols=<sep>,<cls>' + ',<ansColumn/>,<label>' + ' --vocab_size=2000'\n",
    "# spm.SentencePieceTrainer.train(arg_str)\n",
    "\n",
    "spm.SentencePieceTrainer.train(f'--input={m_dir}/tmp.txt --model_prefix={m_dir}/m  --user_defined_symbols=<sep>,<cls>,<pad>   --vocab_size=2000')\n",
    "sp = spm.SentencePieceProcessor()  # model_file='SentencePiece/test_model.model'\n",
    "\n",
    "sp.load(f'{m_dir}/m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁次の問い', '(', '問', '1～3)', 'の会話の', '▁17', '▁～', '▁19', '▁に入れるのに最も適当な', 'ものを', ',', 'それぞれ以下の', '1～4', 'のうちから一つずつ選', 'べ', '。']\n",
      "[70, 32, 29, 188, 609, 197, 79, 218, 169, 64, 20, 93, 59, 66, 58, 35]\n",
      "次の問い(問1～3)の会話の 17 ～ 19 に入れるのに最も適当なものを,それぞれ以下の1～4のうちから一つずつ選べ。\n"
     ]
    }
   ],
   "source": [
    "# encode: text => id\n",
    "tokenized_tokens =  sp.encode_as_pieces('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "print(tokenized_tokens)\n",
    "\n",
    "tokenized_ids = sp.encode_as_ids('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "print(tokenized_ids)\n",
    "\n",
    "decoded_text = sp.decode(tokenized_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <label>問14</label>  <data id=\"D20\" type=\"text\"> <ansColumn id=\"A21\">21</ansColumn></data>  <choices anscol=\"A21\" comment=\"\"> <choice><cNum>①</cNum> Yes, thank you.  </choice> <choice><cNum>②</cNum> Yes, that's O.K.  </choice> <choice><cNum>③</cNum> Not at all.  </choice> <choice><cNum>④</cNum> I'd be glad to.  </choice> </choices> ['▁<', 'label', '>', '問', '14', '</', 'label', '>', '▁<', 'data', '▁id', '=', '\"', 'D', '20', '\"', '▁type', '=', '\"', 'text', '\"', '>', '▁<', 'ansColumn', '▁id', '=', '\"', 'A', '21', '\"', '>21</', 'ansColumn', '></', 'data', '>', '▁<', 'choices', '▁ans', 'col', '=', '\"', 'A', '21', '\"', '▁comment', '=\"\"', '>', '▁<', 'choice', '><', 'c', 'Num', '>1</', 'c', 'Num', '>', '▁Yes', ',', '▁than', 'k', '▁you', '.', '▁</', 'choice', '>', '▁<', 'choice', '><', 'c', 'Num', '>2</', 'c', 'Num', '>', '▁Yes', ',', '▁that', \"'\", 's', '▁', 'O', '.', 'K', '.', '▁</', 'choice', '>', '▁<', 'choice', '><', 'c', 'Num', '>3</', 'c', 'Num', '>', '▁No', 't', '▁at', '▁all', '.', '▁</', 'choice', '>', '▁<', 'choice', '><', 'c', 'Num', '>4</', 'c', 'Num', '>', '▁I', \"'\", 'd', '▁be', '▁', 'g', 'l', 'a', 'd', '▁to', '.', '▁</', 'choice', '>', '▁</', 'choices', '>']\n"
     ]
    }
   ],
   "source": [
    "example_content = df_tmp['contents'][20]\n",
    "print(example_content, sp.encode_as_pieces(example_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MDVc6UGO5Dh6"
   },
   "outputs": [],
   "source": [
    "# for index in encoded_string:\n",
    "#   print('{} ----> {}'.format(index, encoder.decode([index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlYWqhTVlUyQ"
   },
   "source": [
    "## Train 用データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VznrltNOnUc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  2163\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "# 系列を揃えるためのパディング文字列<pad>を追加\n",
    "# パディング文字列のIDは0とする\n",
    "word2index.update({\"<pad>\":0})\n",
    "\n",
    "for inst, cont in zip(df['<instruction/>'], df['contents']):\n",
    "#     try:\n",
    "    tokens = sp.encode_as_pieces(inst + cont)\n",
    "    for word in tokens:\n",
    "            if word in word2index: continue\n",
    "            word2index[word] = len(word2index)\n",
    "#     except TypeError:\n",
    "#         print(f'[Error] <instruction/> が nan です。')\n",
    "#         print(f'    inst : {inst}')\n",
    "#         print(f'    cont : {cont}')\n",
    "\n",
    "print(\"vocab size : \", len(word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIC_O', 'DIC_O, DIS_S, R_ENT', 'DIC_O, DIS_W', 'DIC_O, DIS_W, GK', 'DIC_O, EG', 'DIC_O, EG, DIS_W', 'DIC_O, EG, GK', 'DIC_O, GK', 'DIC_O, Other', 'DIC_O, SEL', 'DIC_O,DIS_W', 'DIC_O,GK', 'DIS_C, GK', 'DIS_O', 'DIS_O, DIS_W', 'DIS_O, DIS_W, GK', 'DIS_O, GK', 'DIS_O, Other', 'DIS_S', 'DIS_S, IC_O', 'DIS_S, R_ENT', 'DIS_S, R_ENT, IC_G', 'DIS_S, R_ENT, IC_M', 'DIS_S, R_ENT, IC_O', 'DIS_S, R_ENT, IC_T', 'DIS_S, R_ENT, R_SUM', 'DIS_S,R_ENT', 'DIS_S,R_ENT,R_SUM', 'DIS_S,R_QA', 'DIS_W', 'DIS_W, GK', 'DIS_W, R_ENT', 'DIS_W, R_ENT, IC_G', 'DIS_W, R_ENT, IC_G, IC_P', 'DIS_W, R_ENT, IC_M', 'DIS_W, R_ENT, IC_O', 'DIS_W, R_ENT, IC_O, IC_P', 'DIS_W, R_ENT, IC_P', 'DIS_W, R_ENT, IC_P, IC_T', 'DIS_W, R_ENT, IC_T', 'DIS_W, R_ENT, R_SUM', 'DIS_W, R_ENT, R_SUM, IC_O', 'DIS_W, R_QA', 'DIS_W,DIC_O', 'DIS_W,EG', 'DIS_W,GK', 'DIS_W,IC_P', 'DIS_W,R_ENT', 'DIS_W,R_ENT,IC_P', 'DIS_W,R_QA', 'DIS_W,R_SUM', 'DIS_W,R_SUM,IC_T', 'EG', 'EG, ', 'EG, DIS_S', 'EG, DIS_W', 'EG, DIS_W, GK', 'EG, GK', 'EG,DIS_W', 'EG,SEL', 'FOC_1', 'FOC_1 ', 'FOC_2', 'GK,Other', 'IC_G,IC_O,MATH_IIB_STATISTICS', 'IC_G,MATH_IIB_STATISTICS', 'IC_O,IC_T,MATH_IIB_STATISTICS', 'IC_O,MATH_IA_PROBABILITY', 'IC_O,MATH_IIB_VECTOR', 'IC_T,DIS_W,R_SUM', 'IC_T,IC_G,MATH_IIB_STATISTICS', 'IC_T,IC_O,MATH_IIB_STATISTICS', 'IC_T,MATH_IIB_COMPUTER', 'IC_T,MATH_IIB_STATISTICS', 'IDM', 'IDM, DIC_O, EG', 'IDM, DIS_W', 'IDM, EG', 'IDM, EG, DIS_S', 'IDM, EG, DIS_W', 'IDM, EG, DIS_W, GK', 'IDM, EG, GK', 'IDM, SEL', 'IDM, SEL, DIS_W', 'IDM, SEL, DIS_W, GK', 'IDM, SEL, EG', 'IDM, SEL, EG, DIS_W', 'IDM, SEL, EG, DIS_W, GK', 'IDM, SEL, EG, GK', 'IDM,DIC_O', 'IDM,DIS_W', 'IDM,GK', 'IDM,SEL,EG,DIS_W', 'IDM,SEL,EG,DIS_W,IC_T', 'MATH_IA_EQ', 'MATH_IA_GEOMETRY', 'MATH_IA_PARABOLA', 'MATH_IA_PROBABILITY', 'MATH_IA_PROBABILITY,IC_O', 'MATH_IA_SET_LOGIC', 'MATH_IA_SET_LOGIC,IC_G', 'MATH_IA_SET_LOGIC,IC_G,IC_O', 'MATH_IA_SET_LOGIC,IC_G,IC_T', 'MATH_IA_SET_LOGIC,IC_T', 'MATH_IIB_CALCULUS', 'MATH_IIB_COMPUTER', 'MATH_IIB_EXPLOG', 'MATH_IIB_GEOMETRY', 'MATH_IIB_NUMSEQ', 'MATH_IIB_POLY', 'MATH_IIB_POLY,MATH_IIB_EXPLOG', 'MATH_IIB_STATISTICS', 'MATH_IIB_STATISTICS,IC_G,IC_T', 'MATH_IIB_TRIGONOMETRY', 'MATH_IIB_VECTOR', 'Other', 'Other,IC_T', 'Other,IC_T,IC_O', 'PRN', 'R_ENT', 'R_ENT, IC_G', 'R_ENT, IC_M', 'R_ENT, IC_O', 'R_ENT, IC_P', 'R_ENT, IC_T', 'R_ENT,DIS_S', 'R_ENT,DIS_W', 'R_ENT,IC_G', 'R_ENT,IC_G,IC_P', 'R_ENT,IC_O', 'R_ENT,IC_P', 'R_ENT,IC_P,DIS_S', 'R_ENT,IC_T', 'R_ENT,IC_T,IC_O', 'R_O, EG, DIS_W', 'R_QA', 'R_QA, IC_G', 'R_QA, IC_M', 'R_QA, IC_O', 'R_QA, IC_P', 'R_QA, IC_T', 'R_QA, R_ENT, R_SUM', 'R_QA, R_SUM', 'R_QA,IC_G', 'R_QA,IC_G,IC_P', 'R_QA,IC_O', 'R_QA,IC_P', 'R_QA,IC_P,DIS_S', 'R_QA,IC_P,IC_T', 'R_QA,IC_T', 'R_QA,IC_T,IC_O', 'R_RA, IC_P', 'R_SUM', 'R_SUM,IC_T', 'SEL', 'SEL, DIC_O', 'SEL, DIC_O, EG', 'SEL, DIS_W', 'SEL, EG', 'SEL, EG ', 'SEL, EG,  DIS_W', 'SEL, EG, DIS_W', 'SEL, EG, GK', 'SEL, GK', 'SEL,DIS_W', 'SEL,EG,DIS_W', 'SEL,EG,DIS_W,IDM', 'SEL,EG,DIS_W,IDM,IC_T']\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "## set_dict から自動抽出する！\n",
    "# attr_name = 'knowledge_type'  # 'answer_type'\n",
    "\n",
    "categories = set()\n",
    "for sbj in sbj_names:\n",
    "    with open(f'../class_set/{sbj}-{elmnt_name}-{attr_name}.json') as f:\n",
    "        categories |= set(json.load(f))   # sbj_names = ['Eigo', ]\n",
    "\n",
    "# print(categories)\n",
    "\n",
    "categories = list(categories)\n",
    "categories.sort()    # 入れないと、クラス番号が変わってしまい、再現実験ができないので注意？\n",
    "print(categories)\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8lgLRE0z4Opm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3016it [00:01, 1722.17it/s]\n",
      "100%|██████████| 3016/3016 [00:00<00:00, 5548.75it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-35bc434c3d51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_datasets_c_xml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_datasets_category\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "## 系列の長さを揃えてバッチでまとめる\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "cat2index = {}\n",
    "for cat in categories:\n",
    "    if cat in cat2index: continue\n",
    "    cat2index[cat] = len(cat2index)\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    # print(tokens)\n",
    "    return [word2index[w] for w in tokens]\n",
    "\n",
    "def category2index(cat):\n",
    "    return cat2index[cat]\n",
    "\n",
    "index_datasets_c_xml_tmp = []\n",
    "index_datasets_category = []\n",
    "\n",
    "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
    "max_len = 0\n",
    "for inst, cont, category in tqdm(zip(df['<instruction/>'], df['contents'], df[attr_name])):\n",
    "    index_c_xml = sentence2index(inst + cont)\n",
    "    index_category = category2index(category)\n",
    "    index_datasets_c_xml_tmp.append(index_c_xml)\n",
    "    index_datasets_category.append(index_category)\n",
    "    if max_len < len(index_c_xml):\n",
    "        max_len = len(index_c_xml)\n",
    "        # if max_len > 10000:\n",
    "        #     print(inst, cont)\n",
    "\n",
    "# 系列の長さを揃えるために短い系列にパディングを追加\n",
    "index_datasets_c_xml = []\n",
    "for c_xml in tqdm(index_datasets_c_xml_tmp):\n",
    "    # パディング作成\n",
    "    padd = [0] * (max_len - len(c_xml))\n",
    "    # 後ろパディングだと正しく学習できなかったので、前パディング\n",
    "    c_xml = padd + c_xml # 前パディング\n",
    "    # c_xml = c_xml + padd # 後ろパディング\n",
    "#     print(len(c_xml))\n",
    "    index_datasets_c_xml.append(c_xml)\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(index_datasets_c_xml, index_datasets_category, train_size=0.7)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_valid = np.array(x_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e75b49e0a75d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mx_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0my_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# 特徴量の標準化\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_valid = scaler.transform(x_valid)\n",
    "\n",
    "# Tensor型に変換\n",
    "# 学習に入れるときはfloat型 or long型になっている必要があるのここで変換してしまう\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_valid = torch.from_numpy(x_valid)\n",
    "y_valid = torch.from_numpy(y_valid)\n",
    "\n",
    "print('x_train : ', x_train.shape)\n",
    "print('y_train : ', y_train.shape)\n",
    "print('x_valid : ', x_valid.shape)\n",
    "print('y_valid : ', y_valid.shape)\n",
    "print(x_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Dataset  ###\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "\n",
    "# 動作確認\n",
    "# indexを指定すればデータを取り出すことができます。\n",
    "index = 0\n",
    "print(train_dataset.__getitem__(index)[0].size())\n",
    "print(train_dataset.__getitem__(index)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "from model_abc.LSTM_text_classify_model import (\n",
    "    LSTM_TextClassifier_ptModel\n",
    ")\n",
    "%autoreload\n",
    "\n",
    "# GPUを使うために必要\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "# 単語の埋め込み次元数上げた。精度がそこそこアップ！ハイパーパラメータのチューニング大事。\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_SIZE = len(categories)\n",
    "\n",
    "## モデルの保存場所を準備する。\n",
    "import datetime\n",
    "dt_now = datetime.datetime.now()\n",
    "save_m_dir = os.path.join('_logs', dt_now.strftime('%Y-%m-%d_%Hh%Mm%Ss'))\n",
    "\n",
    "model = LSTM_TextClassifier_ptModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE,\n",
    "                                        save_m_dir, save_model_name='LSTM_classifier_.pth').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIwH3nto596k"
   },
   "source": [
    "## Experiment Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-52e3e219ba4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase_ExpTrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch_ExpTrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexp_batch_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch_ExpTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore_index=PAD_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from base_ExpTrain import Batch_ExpTrain\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "epochs=1500\n",
    "batch_size=300\n",
    "early_stopping=500\n",
    "\n",
    "exp_batch_train = Batch_ExpTrain(train_dataset, valid_dataset, device)\n",
    "criterion = nn.NLLLoss()  # ignore_index=PAD_token\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "max_valid_acc = exp_batch_train.exec(\n",
    "                            model, criterion, optimizer,\n",
    "                            epochs=epochs, batch_size=batch_size, early_stopping=early_stopping )\n",
    "                            # teacher_forcing=0.5, early_stopping=5)\n",
    "\n",
    "print(max_valid_acc)\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 実験パラメータのメモを保存\n",
    "import json\n",
    "with open(os.path.join(save_m_dir, 'model_params.json'), 'w') as param_f:\n",
    "    json.dump({\n",
    "        'Model' : {\n",
    "            'EMBEDDING_DIM': EMBEDDING_DIM,\n",
    "            'HIDDEN_DIM' : HIDDEN_DIM,\n",
    "            'VOCAB_SIZE' : VOCAB_SIZE,\n",
    "            'TAG_SIZE':TAG_SIZE,\n",
    "        },\n",
    "        'Experiment' : {\n",
    "            'XML条件' : {\n",
    "                'sbj_names': sbj_names,\n",
    "                'elmnt_name': elmnt_name,\n",
    "                'attr_name': attr_name\n",
    "            },\n",
    "           '実験設定' : {\n",
    "                'lr':lr,\n",
    "                'epochs':epochs,\n",
    "                'batch_size':batch_size,\n",
    "                'early_stopping':early_stopping\n",
    "            },\n",
    "            '実験結果' : {\n",
    "                'lr':lr,\n",
    "                'epochs':epochs,\n",
    "                'batch_size':batch_size,\n",
    "                'early_stopping':early_stopping\n",
    "            }\n",
    "        },\n",
    "    }, param_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Acc 計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaNbXi43YgUT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:54<00:00, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict :  0.02983425414364641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_num = len(x_valid)\n",
    "valid_acc = 0\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (X_batch, Y_batch) in enumerate(valid_dataloader):\n",
    "        valid_loss = 0\n",
    "        valid_loss, pred_batch_arr = model.predict(X_batch, Y_batch, criterion, self.device)\n",
    "        # acc を計算する。\n",
    "        _, pred_batch = torch.max(pred_batch_arr, 1)\n",
    "        # acc を計算する。\n",
    "        for j, ans in enumerate(Y_batch):\n",
    "            # print(pred_batch[j].item(), ans.item())\n",
    "            if pred_batch[j].item() == ans.item():\n",
    "                valid_acc += 1\n",
    "#             else:\n",
    "#                 print(predicts[j].item(), ans.item())\n",
    "        total_count += Y_batch.size(0)\n",
    "    valid_acc /= total_count\n",
    "print(\"[Info] acc : \", valid_acc, \"loss : \", valid_loss)\n",
    "# predict :  0.6967916854948034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('tensorflow2.3_py3.8': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}