{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RNN を使ったテキスト分類"
   ],
   "metadata": {
    "id": "9TnJztDZGw-n"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install -q tf-nightly\n",
    "# import tensorflow_datasets as tfds\n",
    "# !pip3 list | grep tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "# !pip3 install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {
    "id": "z682XYsrjkY9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 読み込み元\n",
    "save_m_dir = os.path.join('_logs', \n",
    "                # '_best_weight'\n",
    "                # '0715_13h43m01s_En-k_Bi_400x256'\n",
    "                '0711_17h36m40s_Math-k_Bi_400x256'\n",
    "                # '0708_02h38m30s_En-a'\n",
    "                # '0708_03h42m45s_Math-a'\n",
    "            )\n",
    "\n",
    "with open(os.path.join(save_m_dir, 'model_params.json'), 'r') as param_f:\n",
    "    param_dict = json.load(param_f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n"
   ],
   "metadata": {
    "id": "pRmMubr0jrE2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load"
   ],
   "metadata": {
    "id": "pRmMubr0jrE2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "XML_param = param_dict['Experiment']['XML条件']\n",
    "sbj_names = XML_param['sbj_names']\n",
    "elmnt_name = XML_param['elmnt_name']\n",
    "attr_name = XML_param['attr_name']\n",
    "is_remove_xml = XML_param['is_remove_xml']\n",
    "\n",
    "df = None\n",
    "for sbj in sbj_names:\n",
    "    print(sbj)\n",
    "    attr_csv_path = f'../{sbj}_{attr_name}_ds.tsv'\n",
    "    df_tmp = pd.read_csv(attr_csv_path, delimiter='\\t')\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "print(elmnt_name, elmnt_name)\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()  # nan を削除"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## XMLタグ除去\n",
    "if is_remove_xml:\n",
    "    import re\n",
    "    def remove_xml(xml_child_str):\n",
    "        xml_child_str = re.sub('<.*?>', '', xml_child_str)\n",
    "        return re.sub('</.*?>', '', xml_child_str)\n",
    "\n",
    "    # test_idx = 200\n",
    "    # print(df['contents'][test_idx])\n",
    "    # print(remove_xml(df['contents'][test_idx]))\n",
    "    df['contents'] = df['contents'].map(remove_xml)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize\n",
    "SentencePiece を使用。"
   ],
   "metadata": {
    "id": "pRmMubr0jrE2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "m_dir = '_logs/SentencePiece'\n",
    "os.makedirs(m_dir, exist_ok=True)\n",
    "df.to_csv(f'{m_dir}/tmp.txt', sep='\\t')\n",
    "\n",
    "# arg_str = '--input={m_dir}/tmp.txt --model_prefix={m_dir}/m_user ' + '--user_defined_symbols=<sep>,<cls>' + ',<ansColumn/>,<label>' + ' --vocab_size=2000'\n",
    "# spm.SentencePieceTrainer.train(arg_str)\n",
    "spm.SentencePieceTrainer.train(f'--input={m_dir}/tmp.txt --model_prefix={m_dir}/m  --user_defined_symbols=<sep>,<cls>,<pad>   --vocab_size=2000')\n",
    "sp = spm.SentencePieceProcessor()  # model_file='SentencePiece/test_model.model'\n",
    "\n",
    "sp.load(f'{m_dir}/m.model')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# encode: text => id\n",
    "tokenized_tokens =  sp.encode_as_pieces('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "print(tokenized_tokens)\n",
    "\n",
    "tokenized_ids = sp.encode_as_ids('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "print(tokenized_ids)\n",
    "\n",
    "decoded_text = sp.decode(tokenized_ids)\n",
    "print(decoded_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# example_content = df_tmp['contents'][20]\n",
    "# print(example_content, sp.encode_as_pieces(example_content))\n",
    "\n",
    "# for index in encoded_string:\n",
    "#   print('{} ----> {}'.format(index, encoder.decode([index])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train 用データの準備"
   ],
   "metadata": {
    "id": "GlYWqhTVlUyQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word2index = {}\n",
    "# 系列を揃えるためのパディング文字列<pad>を追加\n",
    "# パディング文字列のIDは0とする\n",
    "word2index.update({\"<pad>\":0})\n",
    "\n",
    "for inst, cont in zip(df['<instruction/>'], df['contents']):\n",
    "#     try:\n",
    "    tokens = sp.encode_as_pieces(inst + cont)\n",
    "    for word in tokens:\n",
    "            if word in word2index: continue\n",
    "            word2index[word] = len(word2index)\n",
    "#     except TypeError:\n",
    "#         print(f'[Error] <instruction/> が nan です。')\n",
    "#         print(f'    inst : {inst}')\n",
    "#         print(f'    cont : {cont}')\n",
    "\n",
    "print(\"vocab size : \", len(word2index))"
   ],
   "outputs": [],
   "metadata": {
    "id": "VznrltNOnUc5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## set_dict から自動抽出する！\n",
    "categories = set()\n",
    "for sbj in sbj_names:\n",
    "    with open(f'../class_set/{sbj}-{elmnt_name}-{attr_name}.json') as f:\n",
    "        categories |= set(json.load(f))   # sbj_names = ['Eigo', ]\n",
    "\n",
    "categories = list(categories)\n",
    "categories.sort()    # 入れないと、クラス番号が変わってしまい、再現実験ができないので注意？\n",
    "print(categories)\n",
    "print(len(categories))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 系列の長さを揃えてバッチでまとめる\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "cat2index = {}\n",
    "for cat in categories:\n",
    "    if cat in cat2index: continue\n",
    "    cat2index[cat] = len(cat2index)\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    # print(tokens)\n",
    "    return [word2index[w] for w in tokens]\n",
    "\n",
    "def category2index(cat):\n",
    "    return cat2index[cat]\n",
    "\n",
    "index_datasets_c_xml_tmp = []\n",
    "index_datasets_category = []\n",
    "\n",
    "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
    "max_len = 0\n",
    "for inst, cont, category in tqdm(zip(df['<instruction/>'], df['contents'], df[attr_name])):\n",
    "    index_c_xml = sentence2index(inst + cont)\n",
    "    index_category = category2index(category)\n",
    "    index_datasets_c_xml_tmp.append(index_c_xml)\n",
    "    index_datasets_category.append(index_category)\n",
    "    if max_len < len(index_c_xml):\n",
    "        max_len = len(index_c_xml)\n",
    "        # if max_len > 10000:\n",
    "        #     print(inst, cont)\n",
    "\n",
    "# 系列の長さを揃えるために短い系列にパディングを追加\n",
    "index_datasets_c_xml = []\n",
    "for c_xml in tqdm(index_datasets_c_xml_tmp):\n",
    "    # パディング作成\n",
    "    padd = [0] * (max_len - len(c_xml))\n",
    "    # 後ろパディングだと正しく学習できなかったので、前パディング\n",
    "    c_xml = padd + c_xml # 前パディング\n",
    "    # c_xml = c_xml + padd # 後ろパディング\n",
    "#     print(len(c_xml))\n",
    "    index_datasets_c_xml.append(c_xml)\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "8lgLRE0z4Opm",
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "# train/valid に分割する？ or 全データを使う？\n",
    "is_split_train_test = [\n",
    "#     True\n",
    "    False\n",
    "][0]\n",
    "\n",
    "if is_split_train_test:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(index_datasets_c_xml, index_datasets_category, train_size=0.7)\n",
    "\n",
    "    x_train = torch.tensor(x_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    x_test = torch.tensor(x_test)\n",
    "    y_test = torch.tensor(y_test)\n",
    "\n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    # x_train = np.array(x_train)\n",
    "    # y_train = np.array(y_train)\n",
    "    # x_test = np.array(x_test)\n",
    "    # y_test = np.array(y_test)\n",
    "    # print(x_train[:5])\n",
    "\n",
    "    # 特徴量の標準化\n",
    "    # scaler = StandardScaler()\n",
    "    # scaler.fit(x_train)\n",
    "    # x_train = scaler.transform(x_train)\n",
    "    # x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Tensor型に変換\n",
    "    # x_train = torch.from_numpy(x_train)\n",
    "    # y_train = torch.from_numpy(y_train)\n",
    "    # x_test = torch.from_numpy(x_test)\n",
    "    # y_test = torch.from_numpy(y_test)\n",
    "\n",
    "    # print('x_train : ', x_train.shape)\n",
    "    # print('y_train : ', y_train.shape)\n",
    "    # print('x_test : ', x_test.shape)\n",
    "    # print('y_test : ', y_test.shape)\n",
    "    # print(x_train[:5])\n",
    "    print(type(x_train))\n",
    "\n",
    "else:\n",
    "    x_test = torch.tensor(index_datasets_c_xml)\n",
    "    y_test = torch.tensor(index_datasets_category)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "###  Dataset  ###\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "if is_split_train_test:\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "\n",
    "# 動作確認\n",
    "index = 0\n",
    "print(len(test_dataset))\n",
    "print(test_dataset.__getitem__(index)[0].size())\n",
    "print(test_dataset.__getitem__(index)[1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "id": "bjUqGVBxGw-t"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "%load_ext autoreload\n",
    "from model_abc.LSTM_text_classify_model import (\n",
    "    LSTM_TextClassifier_ptModel,\n",
    "    BiLSTM_TextClassifier_ptModel\n",
    ")\n",
    "%autoreload\n",
    "\n",
    "# GPUを使うために必要\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# モデルのハイパーパラメータ\n",
    "model_param = param_dict['Model']\n",
    "EMBEDDING_DIM = model_param['EMBEDDING_DIM']\n",
    "HIDDEN_DIM = model_param['HIDDEN_DIM']\n",
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_SIZE = len(categories)\n",
    "MODEL_NAME = model_param['MODEL_NAME']\n",
    "\n",
    "## モデルの保存場所を準備する。\n",
    "def get_model():\n",
    "    if MODEL_NAME == 'LSTM':\n",
    "        model = LSTM_TextClassifier_ptModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
    "    elif MODEL_NAME == 'BiLSTM':\n",
    "        model = BiLSTM_TextClassifier_ptModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
    "    return model\n",
    "\n",
    "# model = LSTM_TextClassifier_ptModel(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)"
   ],
   "outputs": [],
   "metadata": {
    "id": "LwfoBkmRYcP3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Evaluate"
   ],
   "metadata": {
    "id": "zIwH3nto596k"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## モデル読み込み\n",
    "opt = ''  #+ 'latest'\n",
    "\n",
    "model = get_model()\n",
    "model.load_weights(\n",
    "        load_m_path=f'{save_m_dir}/model_weghts{opt}.pth')\n",
    "\n",
    "import torch.nn as nn\n",
    "criterion = nn.NLLLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from collections import deque\n",
    "\n",
    "test_acc = 0\n",
    "total_count = 0\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
    "\n",
    "y_ans_list = []\n",
    "y_pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (X_batch, Y_batch) in enumerate(test_dataloader):\n",
    "        test_loss = 0\n",
    "        test_loss, pred_batch_arr = model.predict(X_batch, Y_batch, criterion, device)\n",
    "        # acc を計算する。\n",
    "        _, pred_batch = torch.max(pred_batch_arr, 1)\n",
    "        for j, ans in enumerate(Y_batch):\n",
    "            if pred_batch[j].item() == ans.item():\n",
    "                test_acc += 1\n",
    "            else:\n",
    "                print(categories[predicts[j].item()], ans.item())\n",
    "        total_count += Y_batch.size(0)\n",
    "        y_ans_list += Y_batch.tolist()\n",
    "        y_pred_list += pred_batch.tolist()\n",
    "    test_acc /= total_count\n",
    "\n",
    "print(f\"[Info] acc : {test_acc},  loss : {test_loss}\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "BaNbXi43YgUT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(f\"[Info] y_ans_list : {y_ans_list}\")\n",
    "# print(f\"[Info] y_pred_list : {y_pred_list}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Analyze\n",
    "- [x]  （棒グラフ）「クラスごとのデータ数」を作る。\n",
    "- [x]  （ヒートマップ）「混同行列」 を出す（おそらく、かなりの偏りがあるはず）\n",
    "- [x]  よく間違えているデータの列挙。\n",
    "- [ ]  Attention などを挟んで、注目単語を可視化する？"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "do_analyze_graph = [\n",
    "    # True\n",
    "    False\n",
    "][0]\n",
    "\n",
    "do_analyze_statistic = [\n",
    "    True\n",
    "    # False\n",
    "][0]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### （棒グラフ）「クラスごとのデータ数」"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if do_analyze:\n",
    "    w = len(categories)\n",
    "    h = w\n",
    "    plt.figure(figsize=(w,h))\n",
    "\n",
    "    g = sns.countplot(x=attr_name, data=df, \n",
    "                      order=df[attr_name].value_counts().index)\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if do_analyze:\n",
    "    g = sns.countplot(x=attr_name, data=df, \n",
    "                      order=categories)\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=90)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# classification_report\n",
    "report = classification_report(\n",
    "                y_ans_list, y_pred_list, \n",
    "                # labels=categories\n",
    "            )\n",
    "print(report)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### （ヒートマップ）「混同行列」 を出す"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm = confusion_matrix(y_ans_list, y_pred_list)\n",
    "cm = pd.DataFrame(data=cm, index=categories, columns=categories)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if do_analyze:\n",
    "    w = len(categories)\n",
    "    h = w * 7 / 10\n",
    "    plt.figure(figsize=(w,h))\n",
    "    sns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')\n",
    "    plt.savefig('sklearn_confusion_matrix.png')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### よく間違えているデータの列挙。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "___"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('tensorflow2.3_py3.8': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}