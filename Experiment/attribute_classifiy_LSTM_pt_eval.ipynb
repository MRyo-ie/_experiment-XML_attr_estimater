{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "0nbI5DtDGw-i"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# RNN を使ったテキスト分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "z682XYsrjkY9"
   },
   "outputs": [],
   "source": [
    "# !pip install -q tf-nightly\n",
    "# import tensorflow_datasets as tfds\n",
    "# !pip3 list | grep tensorflow\n",
    "# import tensorflow as tf\n",
    "\n",
    "# !pip3 install sentencepiece\n",
    "import sentencepiece as spm\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# GPUを使うために必要\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mp1Z7P9pYRSK"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_graphs(history, metric):\n",
    "#   plt.plot(history.history[metric])\n",
    "#   plt.plot(history.history['val_'+metric], '')\n",
    "#   plt.xlabel(\"Epochs\")\n",
    "#   plt.ylabel(metric)\n",
    "#   plt.legend([metric, 'val_'+metric])\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigo\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>knowledge_type</th>\n",
       "      <th>&lt;instruction/&gt;</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問１&lt;/label&gt; &lt;ansColumn id=\"A1\"&gt;1&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問２&lt;/label&gt; &lt;ansColumn id=\"A2\"&gt;2&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問３&lt;/label&gt; &lt;ansColumn id=\"A3\"&gt;3&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問４&lt;/label&gt; &lt;ansColumn id=\"A4\"&gt;4&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>&lt;label&gt;問５&lt;/label&gt; &lt;ansColumn id=\"A5\"&gt;5&lt;/ansC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>2630</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>&lt;label&gt;問２&lt;/label&gt;  &lt;data id=\"D44\" type=\"text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>2631</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>&lt;label&gt;問３&lt;/label&gt;  &lt;data id=\"D45\" type=\"text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>2632</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>&lt;label&gt;問４&lt;/label&gt;  &lt;data id=\"D46\" type=\"text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>2633</td>\n",
       "      <td>Other</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>&lt;label&gt;問５&lt;/label&gt;  &lt;data id=\"D47\" type=\"text...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>2634</td>\n",
       "      <td>R_SUM,IC_T</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>&lt;label&gt;Ｂ&lt;/label&gt;&lt;instruction&gt;  次の  &lt;ref target...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2635 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 knowledge_type  \\\n",
       "0              0            PRN   \n",
       "1              1            PRN   \n",
       "2              2            PRN   \n",
       "3              3            PRN   \n",
       "4              4            PRN   \n",
       "...          ...            ...   \n",
       "2630        2630           R_QA   \n",
       "2631        2631           R_QA   \n",
       "2632        2632           R_QA   \n",
       "2633        2633          Other   \n",
       "2634        2634     R_SUM,IC_T   \n",
       "\n",
       "                                         <instruction/>  \\\n",
       "0     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "1     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "2     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "3     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "4     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "...                                                 ...   \n",
       "2630  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2631  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2632  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2633  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2634  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "\n",
       "                                               contents  \n",
       "0       <label>問１</label> <ansColumn id=\"A1\">1</ansC...  \n",
       "1       <label>問２</label> <ansColumn id=\"A2\">2</ansC...  \n",
       "2       <label>問３</label> <ansColumn id=\"A3\">3</ansC...  \n",
       "3       <label>問４</label> <ansColumn id=\"A4\">4</ansC...  \n",
       "4       <label>問５</label> <ansColumn id=\"A5\">5</ansC...  \n",
       "...                                                 ...  \n",
       "2630    <label>問２</label>  <data id=\"D44\" type=\"text...  \n",
       "2631    <label>問３</label>  <data id=\"D45\" type=\"text...  \n",
       "2632    <label>問４</label>  <data id=\"D46\" type=\"text...  \n",
       "2633    <label>問５</label>  <data id=\"D47\" type=\"text...  \n",
       "2634  <label>Ｂ</label><instruction>  次の  <ref target...  \n",
       "\n",
       "[2635 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbj_names = [\n",
    "    ['Eigo']\n",
    "#     ['Suugaku']\n",
    "    # ['Eigo', 'Suugaku']\n",
    "][0]\n",
    "elmnt_name = 'question'\n",
    "attr_name = 'knowledge_type'  # 'answer_type'   #\n",
    "is_remove_xml = True #@param {type:\"boolean\"}\n",
    "\n",
    "df = None\n",
    "for sbj in sbj_names:\n",
    "    print(sbj)\n",
    "    attr_csv_path = f'../{sbj}_{attr_name}_ds.tsv'\n",
    "    df_tmp = pd.read_csv(attr_csv_path, delimiter='\\t')\n",
    "    df = pd.concat([df, df_tmp])\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.dropna()  # nan を削除\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SHRwRoP2nVHX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>knowledge_type</th>\n",
       "      <th>&lt;instruction/&gt;</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>問１ 1 ① circumstance   ② pursue   ③ universal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>問２ 2 ① definite   ② delicate   ③ recently   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>問３ 3 ① patience   ② magic   ③ manager   ④ ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>問４ 4 ① conference   ② confidence   ③ contine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PRN</td>\n",
       "      <td>次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...</td>\n",
       "      <td>問５ 5 ① photograph   ② profit   ③ notice   ④ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>2630</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>問２   According to paragraph (3) , what did p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>2631</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>問３   According to paragraph (4) , which opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2632</th>\n",
       "      <td>2632</td>\n",
       "      <td>R_QA</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>問４   According to paragraph (6) , what is on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>2633</td>\n",
       "      <td>Other</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>問５  What would be the best title for this pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>2634</td>\n",
       "      <td>R_SUM,IC_T</td>\n",
       "      <td>次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...</td>\n",
       "      <td>Ｂ  次の  表  は，本文のパラグラフ（段落）ごとの内容をまとめたものである。  52  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2635 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 knowledge_type  \\\n",
       "0              0            PRN   \n",
       "1              1            PRN   \n",
       "2              2            PRN   \n",
       "3              3            PRN   \n",
       "4              4            PRN   \n",
       "...          ...            ...   \n",
       "2630        2630           R_QA   \n",
       "2631        2631           R_QA   \n",
       "2632        2632           R_QA   \n",
       "2633        2633          Other   \n",
       "2634        2634     R_SUM,IC_T   \n",
       "\n",
       "                                         <instruction/>  \\\n",
       "0     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "1     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "2     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "3     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "4     次の問い(問１～５)のそれぞれの単語①～④のうちから，アクセント(第一強勢)のある母音の発音...   \n",
       "...                                                 ...   \n",
       "2630  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2631  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2632  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2633  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "2634  次の問い（問１～５）の 47 ～ 51 に入れるのに最も適当なものを，それぞれ下の①～④のう...   \n",
       "\n",
       "                                               contents  \n",
       "0       問１ 1 ① circumstance   ② pursue   ③ universal...  \n",
       "1       問２ 2 ① definite   ② delicate   ③ recently   ...  \n",
       "2       問３ 3 ① patience   ② magic   ③ manager   ④ ba...  \n",
       "3       問４ 4 ① conference   ② confidence   ③ contine...  \n",
       "4       問５ 5 ① photograph   ② profit   ③ notice   ④ ...  \n",
       "...                                                 ...  \n",
       "2630    問２   According to paragraph (3) , what did p...  \n",
       "2631    問３   According to paragraph (4) , which opin...  \n",
       "2632    問４   According to paragraph (6) , what is on...  \n",
       "2633    問５  What would be the best title for this pa...  \n",
       "2634  Ｂ  次の  表  は，本文のパラグラフ（段落）ごとの内容をまとめたものである。  52  ...  \n",
       "\n",
       "[2635 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## XMLタグ除去\n",
    "if is_remove_xml:\n",
    "    import re\n",
    "    def remove_xml(xml_child_str):\n",
    "        xml_child_str = re.sub('<.*?>', '', xml_child_str)\n",
    "        return re.sub('</.*?>', '', xml_child_str)\n",
    "\n",
    "    # test_idx = 200\n",
    "    # print(df['contents'][test_idx])\n",
    "    # print(remove_xml(df['contents'][test_idx]))\n",
    "    df['contents'] = df['contents'].map(remove_xml)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRmMubr0jrE2"
   },
   "source": [
    "### Tokenize\n",
    "SentencePiece を使用。\n",
    "- タグ あり／なし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tmp = df[['<instruction/>', 'contents']]\n",
    "# df_tmp['<instruction/>'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_dir = '_logs/SentencePiece'\n",
    "os.makedirs(m_dir, exist_ok=True)\n",
    "df.to_csv(f'{m_dir}/tmp.txt', sep='\\t')\n",
    "\n",
    "# arg_str = '--input={m_dir}/tmp.txt --model_prefix={m_dir}/m_user ' + '--user_defined_symbols=<sep>,<cls>' + ',<ansColumn/>,<label>' + ' --vocab_size=2000'\n",
    "# spm.SentencePieceTrainer.train(arg_str)\n",
    "\n",
    "spm.SentencePieceTrainer.train(f'--input={m_dir}/tmp.txt --model_prefix={m_dir}/m  --user_defined_symbols=<sep>,<cls>,<pad>   --vocab_size=2000')\n",
    "sp = spm.SentencePieceProcessor()  # model_file='SentencePiece/test_model.model'\n",
    "\n",
    "sp.load(f'{m_dir}/m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # encode: text => id\n",
    "# tokenized_tokens =  sp.encode_as_pieces('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "# print(tokenized_tokens)\n",
    "\n",
    "# tokenized_ids = sp.encode_as_ids('次の問い(問１～３)の会話の 17 ～ 19 に入れるのに最も適当なものを，それぞれ以下の①～④のうちから一つずつ選べ。\t')\n",
    "# print(tokenized_ids)\n",
    "\n",
    "# decoded_text = sp.decode(tokenized_ids)\n",
    "# print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_content = df_tmp['contents'][20]\n",
    "# print(example_content, sp.encode_as_pieces(example_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MDVc6UGO5Dh6"
   },
   "outputs": [],
   "source": [
    "# for index in encoded_string:\n",
    "#   print('{} ----> {}'.format(index, encoder.decode([index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlYWqhTVlUyQ"
   },
   "source": [
    "## Train 用データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VznrltNOnUc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size :  2100\n"
     ]
    }
   ],
   "source": [
    "word2index = {}\n",
    "# 系列を揃えるためのパディング文字列<pad>を追加\n",
    "# パディング文字列のIDは0とする\n",
    "word2index.update({\"<pad>\":0})\n",
    "\n",
    "for inst, cont in zip(df['<instruction/>'], df['contents']):\n",
    "#     try:\n",
    "    tokens = sp.encode_as_pieces(inst + cont)\n",
    "    for word in tokens:\n",
    "            if word in word2index: continue\n",
    "            word2index[word] = len(word2index)\n",
    "#     except TypeError:\n",
    "#         print(f'[Error] <instruction/> が nan です。')\n",
    "#         print(f'    inst : {inst}')\n",
    "#         print(f'    cont : {cont}')\n",
    "\n",
    "print(\"vocab size : \", len(word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DIC_O', 'DIC_O, DIS_S, R_ENT', 'DIC_O, DIS_W', 'DIC_O, DIS_W, GK', 'DIC_O, EG', 'DIC_O, EG, DIS_W', 'DIC_O, EG, GK', 'DIC_O, GK', 'DIC_O, Other', 'DIC_O, SEL', 'DIC_O,DIS_W', 'DIC_O,GK', 'DIS_C, GK', 'DIS_O', 'DIS_O, DIS_W', 'DIS_O, DIS_W, GK', 'DIS_O, GK', 'DIS_O, Other', 'DIS_S', 'DIS_S, IC_O', 'DIS_S, R_ENT', 'DIS_S, R_ENT, IC_G', 'DIS_S, R_ENT, IC_M', 'DIS_S, R_ENT, IC_O', 'DIS_S, R_ENT, IC_T', 'DIS_S, R_ENT, R_SUM', 'DIS_S,R_ENT', 'DIS_S,R_ENT,R_SUM', 'DIS_S,R_QA', 'DIS_W', 'DIS_W, GK', 'DIS_W, R_ENT', 'DIS_W, R_ENT, IC_G', 'DIS_W, R_ENT, IC_G, IC_P', 'DIS_W, R_ENT, IC_M', 'DIS_W, R_ENT, IC_O', 'DIS_W, R_ENT, IC_O, IC_P', 'DIS_W, R_ENT, IC_P', 'DIS_W, R_ENT, IC_P, IC_T', 'DIS_W, R_ENT, IC_T', 'DIS_W, R_ENT, R_SUM', 'DIS_W, R_ENT, R_SUM, IC_O', 'DIS_W, R_QA', 'DIS_W,DIC_O', 'DIS_W,EG', 'DIS_W,GK', 'DIS_W,IC_P', 'DIS_W,R_ENT', 'DIS_W,R_ENT,IC_P', 'DIS_W,R_QA', 'DIS_W,R_SUM', 'DIS_W,R_SUM,IC_T', 'EG', 'EG, ', 'EG, DIS_S', 'EG, DIS_W', 'EG, DIS_W, GK', 'EG, GK', 'EG,DIS_W', 'EG,SEL', 'FOC_1', 'FOC_1 ', 'FOC_2', 'GK,Other', 'IC_T,DIS_W,R_SUM', 'IDM', 'IDM, DIC_O, EG', 'IDM, DIS_W', 'IDM, EG', 'IDM, EG, DIS_S', 'IDM, EG, DIS_W', 'IDM, EG, DIS_W, GK', 'IDM, EG, GK', 'IDM, SEL', 'IDM, SEL, DIS_W', 'IDM, SEL, DIS_W, GK', 'IDM, SEL, EG', 'IDM, SEL, EG, DIS_W', 'IDM, SEL, EG, DIS_W, GK', 'IDM, SEL, EG, GK', 'IDM,DIC_O', 'IDM,DIS_W', 'IDM,GK', 'IDM,SEL,EG,DIS_W', 'IDM,SEL,EG,DIS_W,IC_T', 'Other', 'Other,IC_T', 'Other,IC_T,IC_O', 'PRN', 'R_ENT', 'R_ENT, IC_G', 'R_ENT, IC_M', 'R_ENT, IC_O', 'R_ENT, IC_P', 'R_ENT, IC_T', 'R_ENT,DIS_S', 'R_ENT,DIS_W', 'R_ENT,IC_G', 'R_ENT,IC_G,IC_P', 'R_ENT,IC_O', 'R_ENT,IC_P', 'R_ENT,IC_P,DIS_S', 'R_ENT,IC_T', 'R_ENT,IC_T,IC_O', 'R_O, EG, DIS_W', 'R_QA', 'R_QA, IC_G', 'R_QA, IC_M', 'R_QA, IC_O', 'R_QA, IC_P', 'R_QA, IC_T', 'R_QA, R_ENT, R_SUM', 'R_QA, R_SUM', 'R_QA,IC_G', 'R_QA,IC_G,IC_P', 'R_QA,IC_O', 'R_QA,IC_P', 'R_QA,IC_P,DIS_S', 'R_QA,IC_P,IC_T', 'R_QA,IC_T', 'R_QA,IC_T,IC_O', 'R_RA, IC_P', 'R_SUM', 'R_SUM,IC_T', 'SEL', 'SEL, DIC_O', 'SEL, DIC_O, EG', 'SEL, DIS_W', 'SEL, EG', 'SEL, EG ', 'SEL, EG,  DIS_W', 'SEL, EG, DIS_W', 'SEL, EG, GK', 'SEL, GK', 'SEL,DIS_W', 'SEL,EG,DIS_W', 'SEL,EG,DIS_W,IDM', 'SEL,EG,DIS_W,IDM,IC_T']\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "## set_dict から自動抽出する！\n",
    "# attr_name = 'knowledge_type'  # 'answer_type'\n",
    "\n",
    "categories = set()\n",
    "for sbj in sbj_names:\n",
    "    with open(f'../class_set/{sbj}-{elmnt_name}-{attr_name}.json') as f:\n",
    "        categories |= set(json.load(f))   # sbj_names = ['Eigo', ]\n",
    "\n",
    "# print(categories)\n",
    "\n",
    "# categories = [\n",
    "#     'sentence', \n",
    "#     'term_person', 'term_location', 'term_time', 'term_other',\n",
    "#     'referenceSymbol',\n",
    "#     'image_graph', 'image_photo', 'image_map', 'image_table', 'image_other',\n",
    "#     'formula', \n",
    "#     'orthography',\n",
    "#     'other',\n",
    "#     # 組み合わせ系（仮追加）\n",
    "#     '(symbol-sentence)*2', '(symbol-sentence)*3', '(symbol-sentence)*4', '(symbol-term_location)*3', '(symbol-term_other)*3', '(symbol-term_other)*3',\n",
    "#     '(symbol-symbol)*4',\n",
    "#     '(term_location-term_location-term_location)', 'term_location-term_location-term_location-term_location',\n",
    "#     '(term_location-term_location-term_location)', '(term_location-term_location-term_location)',\n",
    "#     '(term_other-term_other-term_other)',\n",
    "#     'term_other-term_other-term_other',\n",
    "#     'sentence-sentence',\n",
    "#     'sentence-sentence-sentence',\n",
    "#     'symbol-symbol',\n",
    "#     'symbol-symbol-symbol',\n",
    "#     'symbol-symbol-symbol-symbol',\n",
    "#     'o(symbol-symbol-symbol-symbol)',\n",
    "#     'o(symbol-symbol-symbol)',\n",
    "# ]\n",
    "\n",
    "categories = list(categories)\n",
    "categories.sort()    # 入れないと、クラス番号が変わってしまい、再現実験ができないので注意？\n",
    "print(categories)\n",
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8lgLRE0z4Opm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 系列の長さを揃えてバッチでまとめる\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "cat2index = {}\n",
    "for cat in categories:\n",
    "    if cat in cat2index: continue\n",
    "    cat2index[cat] = len(cat2index)\n",
    "\n",
    "def sentence2index(sentence):\n",
    "    tokens = sp.encode_as_pieces(sentence)\n",
    "    # print(tokens)\n",
    "    return [word2index[w] for w in tokens]\n",
    "\n",
    "def category2index(cat):\n",
    "    return [cat2index[cat]]\n",
    "\n",
    "index_datasets_c_xml_tmp = []\n",
    "index_datasets_category = []\n",
    "\n",
    "# 系列の長さの最大値を取得。この長さに他の系列の長さをあわせる\n",
    "max_len = 0\n",
    "for inst, cont, category in zip(df['<instruction/>'], df['contents'], df[attr_name]):\n",
    "    index_c_xml = sentence2index(inst + cont)\n",
    "    index_category = category2index(category)\n",
    "    index_datasets_c_xml_tmp.append(index_c_xml)\n",
    "    index_datasets_category.append(index_category)\n",
    "    if max_len < len(index_c_xml):\n",
    "        max_len = len(index_c_xml)\n",
    "\n",
    "# 系列の長さを揃えるために短い系列にパディングを追加\n",
    "# 後ろパディングだと正しく学習できなかったので、前パディング\n",
    "index_datasets_c_xml = []\n",
    "for c_xml in index_datasets_c_xml_tmp:\n",
    "    for i in range(max_len - len(c_xml)):\n",
    "        c_xml.insert(0, 0) # 前パディング\n",
    "#     c_xml.append(0)　# 後ろパディング\n",
    "    index_datasets_c_xml.append(c_xml)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(index_datasets_c_xml, index_datasets_category, train_size=0.7)\n",
    "\n",
    "# データをバッチでまとめるための関数\n",
    "def train2batch(c_xml, category, batch_size=100):\n",
    "    c_xml_batch = []\n",
    "    category_batch = []\n",
    "    c_xml_shuffle, category_shuffle = shuffle(c_xml, category)\n",
    "    for i in range(0, len(c_xml), batch_size):\n",
    "        c_xml_batch.append(c_xml_shuffle[i:i+batch_size])\n",
    "        category_batch.append(category_shuffle[i:i+batch_size])\n",
    "    return c_xml_batch, category_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # <pad>の単語IDが0なので、padding_idx=0としている\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # batch_first=Trueが大事！\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        #embeds.size() = (batch_size × len(sentence) × embedding_dim)\n",
    "        _, lstm_out = self.lstm(embeds)\n",
    "        # lstm_out[0].size() = (1 × batch_size × hidden_dim)\n",
    "        tag_space = self.hidden2tag(lstm_out[0])\n",
    "        # tag_space.size() = (1 × batch_size × tagset_size)\n",
    "\n",
    "        # (batch_size × tagset_size)にするためにsqueeze()する\n",
    "        tag_scores = self.softmax(tag_space.squeeze())\n",
    "        # tag_scores.size() = (batch_size × tagset_size)\n",
    "\n",
    "        return tag_scores  \n",
    "\n",
    "    def load_weights(self, load_m_path='_logs/test/LSTM_classifier.pth',):\n",
    "        if load_m_path is not None:\n",
    "            param = torch.load(load_m_path)\n",
    "            self.load_state_dict(param)\n",
    "            print(f'[info] {load_m_path} loaded !')\n",
    "\n",
    "    def save(self, save_f_path='_logs/test/LSTM_classifier.pth',):\n",
    "        torch.save(self.state_dict(), save_f_path)\n",
    "\n",
    "    \n",
    "# 単語の埋め込み次元数上げた。精度がそこそこアップ！ハイパーパラメータのチューニング大事。\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 128\n",
    "VOCAB_SIZE = len(word2index)\n",
    "TAG_SIZE = len(categories)\n",
    "\n",
    "# model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE).to(device)\n",
    "\n",
    "# loss_function = nn.NLLLoss()\n",
    "# # SGDからAdamに変更。特に意味はなし\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-326e550b722c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'_logs/{best_m_dir}/LSTM_classifier{option}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# to(device)でモデルがGPU対応する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f83100011684>\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, load_m_path)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_m_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_logs/test/LSTM_classifier.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_m_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[info] {load_m_path} loaded !'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.4/envs/tensorflow2.3_py3.8/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    136\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "option = ''\n",
    "\n",
    "# 重み読み込み\n",
    "best_m_dir = [\n",
    "#     'best-kwlg_type_sbj_en'\n",
    "#     'best-kwlg_type_sbj_math'\n",
    "    'best-kwlg_type_sbj_en_noXML'\n",
    "    # 'best-kwlg_type_sbj_x2'\n",
    "][0]\n",
    "# option = '_500epc'\n",
    "\n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE)\n",
    "model.load_weights(f'_logs/{best_m_dir}/LSTM_classifier{option}.pth')\n",
    "# to(device)でモデルがGPU対応する\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaNbXi43YgUT"
   },
   "outputs": [],
   "source": [
    "test_num = len(test_x)\n",
    "a = 0\n",
    "with torch.no_grad():\n",
    "    title_batch, category_batch = train2batch(test_x, test_y)\n",
    "\n",
    "    for i in range(len(title_batch)):\n",
    "        title_tensor = torch.tensor(title_batch[i], device=device)\n",
    "        category_tensor = torch.tensor(category_batch[i], device=device)\n",
    "\n",
    "        out = model(title_tensor)\n",
    "        _, predicts = torch.max(out, 1)\n",
    "        for j, ans in enumerate(category_tensor):\n",
    "            if predicts[j].item() == ans.item():\n",
    "                a += 1\n",
    "#             else:\n",
    "                print(predicts[j].item(), ans.item())\n",
    "print(\"predict : \", a / test_num)\n",
    "# predict :  0.6967916854948034"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification_rnn.ipynb",
   "toc_visible": true
  },
  "interpreter": {
   "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4 64-bit ('tensorflow2.3_py3.8': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "089bc7a4b5bcca8ded8f56dd6d31f99db98f335bd2546ffdf0f141ab8351be05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}